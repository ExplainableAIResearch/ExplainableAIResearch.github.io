---
layout: publication
title: "Explaining Explanations in AI"
authors: Brent Mittelstadt, Chris Russell, Sandra Wachter
conference: 
year: 2018
additional_links: 
   - {name: "ArXiv", url: "http://dx.doi.org/10.1145/3287560.3287574"}
tags: []
---
Recent work on interpretability in machine learning and AI has focused on the
building of simplified models that approximate the true criteria used to make
decisions. These models are a useful pedagogical device for teaching trained
professionals how to predict what decisions will be made by the complex system,
and most importantly how the system might break. However, when considering any
such model it's important to remember Box's maxim that "All models are wrong
but some are useful." We focus on the distinction between these models and
explanations in philosophy and sociology. These models can be understood as a
"do it yourself kit" for explanations, allowing a practitioner to directly
answer "what if questions" or generate contrastive explanations without
external assistance. Although a valuable ability, giving these models as
explanations appears more difficult than necessary, and other forms of
explanation may not have the same trade-offs. We contrast the different schools
of thought on what makes an explanation, and suggest that machine learning
might benefit from viewing the problem more broadly.